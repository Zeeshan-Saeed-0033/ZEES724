{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0uZAK2itA2h",
        "outputId": "dd6bdfa1-dc1b-405a-f5b6-637551d1a67a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total GPT-3 Parameters: 175,186,010,112 (~175.19B)\n"
          ]
        }
      ],
      "source": [
        "def calculate_gpt_parameters(\n",
        "    num_layers: int,\n",
        "    hidden_size: int,\n",
        "    vocab_size: int,\n",
        "    ff_multiplier: int = 4\n",
        ") -> int:\n",
        "    \"\"\"\n",
        "    Calculates total number of parameters in a GPT model using Q-K-V matrix approach.\n",
        "\n",
        "    Parameters:\n",
        "        num_layers (int): Number of transformer blocks.\n",
        "        hidden_size (int): Dimensionality of the hidden embeddings (d_model).\n",
        "        vocab_size (int): Size of the tokenizer vocabulary.\n",
        "        ff_multiplier (int): Expansion factor for feedforward network (default = 4).\n",
        "\n",
        "    Returns:\n",
        "        int: Total number of parameters.\n",
        "    \"\"\"\n",
        "\n",
        "    d = hidden_size\n",
        "    d_ff = ff_multiplier * d\n",
        "\n",
        "    # Attention: Q, K, V, and output projection\n",
        "    attention_params = 4 * d * d\n",
        "\n",
        "    # Feedforward Network: two linear layers\n",
        "    ffn_params = 2 * d * d_ff\n",
        "\n",
        "    # LayerNorm: gain and bias for 2 norms per block\n",
        "    layernorm_params = 4 * d\n",
        "\n",
        "    # Per transformer block\n",
        "    per_layer_params = attention_params + ffn_params + layernorm_params\n",
        "\n",
        "    # Total for all layers\n",
        "    transformer_total = num_layers * per_layer_params\n",
        "\n",
        "    # Token embedding + output projection (usually tied/shared)\n",
        "    embedding_total = 2 * vocab_size * d\n",
        "\n",
        "    # Final total\n",
        "    total_params = transformer_total + embedding_total\n",
        "\n",
        "    return total_params\n",
        "\n",
        "\n",
        "# Example: GPT-3 175B\n",
        "num_layers = 96\n",
        "hidden_size = 12288\n",
        "vocab_size = 50257\n",
        "\n",
        "total_params = calculate_gpt_parameters(num_layers, hidden_size, vocab_size)\n",
        "print(f\"Total GPT-3 Parameters: {total_params:,} (~{total_params/1e9:.2f}B)\")\n"
      ]
    }
  ]
}